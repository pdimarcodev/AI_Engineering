{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XAI Project - Neural Network Interpretation for Banking Regulatory Compliance\n",
    "\n",
    "## Project Overview\n",
    "This project implements Explainable AI (XAI) techniques to improve transparency and comprehensibility of neural network models for regulatory compliance in banking. We analyze a pre-trained classification model using various XAI techniques including Grad-CAM, LIME, SHAP, Integrated Gradients, and Occlusion Maps.\n",
    "\n",
    "## Phase 1: Pre-trained Classification Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load MNIST dataset with transforms for DenseNet (requires 3-channel input)\ntransform = transforms.Compose([\n    transforms.Resize(224),  # DenseNet expects 224x224 input\n    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel for DenseNet\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n])\n\ntrain_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Test samples: {len(test_dataset)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-trained DenseNet model from torchvision\nmodel = models.densenet121(pretrained=True)\n\n# Modify classifier for MNIST (10 classes)\nnum_features = model.classifier.in_features\nmodel.classifier = nn.Linear(num_features, 10)\n\nprint(\"Using pre-trained DenseNet-121 model\")\nprint(f\"Modified classifier for {10} classes\")\nprint(model.classifier)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fine-tune the DenseNet model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Freeze feature extraction layers, only train classifier\nfor param in model.features.parameters():\n    param.requires_grad = False\n\noptimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\ndef train_model(model, train_loader, optimizer, criterion, epochs=2):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            \n            if batch_idx % 100 == 0:\n                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.6f}')\n        \n        print(f'Epoch {epoch+1} completed, Average Loss: {running_loss/len(train_loader):.6f}')\n\ntrain_model(model, train_loader, optimizer, criterion)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model accuracy\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}% ({correct}/{total})')\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Saliency Map Generation Using XAI Techniques\n",
    "\n",
    "### Installing Captum for XAI implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install captum with specific numpy version to avoid conflicts\n# Option 1: Install without dependencies and add required ones manually\n!pip install captum --no-deps\n!pip install typing_extensions\n\n# Option 2: Alternative - Install specific compatible versions\n# !pip install \"captum>=0.6.0\" \"numpy>=1.26.0,<2.0.0\" --force-reinstall\n\n# Option 3: If you prefer to use the current environment's numpy\n# Just import captum - it should work with numpy 2.x in most cases\n# !pip install captum --no-deps"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import XAI libraries\n",
    "from captum.attr import (\n",
    "    IntegratedGradients,\n",
    "    Saliency,\n",
    "    GradientShap,\n",
    "    Occlusion,\n",
    "    LayerGradCam\n",
    ")\n",
    "from captum.attr import visualization as viz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample images for analysis\n",
    "def get_sample_images(test_loader, num_samples=10):\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            if len(samples) >= num_samples:\n",
    "                return samples\n",
    "            \n",
    "            samples.append({\n",
    "                'image': data[i],\n",
    "                'true_label': target[i].item(),\n",
    "                'predicted_label': pred[i].item(),\n",
    "                'confidence': torch.softmax(output[i], dim=0).max().item(),\n",
    "                'correct': target[i].item() == pred[i].item()\n",
    "            })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "sample_images = get_sample_images(test_loader, 10)\n",
    "print(f\"Collected {len(sample_images)} sample images\")\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_samples = [s for s in sample_images if s['correct']]\n",
    "incorrect_samples = [s for s in sample_images if not s['correct']]\n",
    "\n",
    "print(f\"Correct predictions: {len(correct_samples)}\")\n",
    "print(f\"Incorrect predictions: {len(incorrect_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_integrated_gradients(model, image, target_class):\n",
    "    integrated_gradients = IntegratedGradients(model)\n",
    "    attributions = integrated_gradients.attribute(\n",
    "        image.unsqueeze(0), \n",
    "        target=target_class,\n",
    "        n_steps=50\n",
    "    )\n",
    "    return attributions.squeeze()\n",
    "\n",
    "# Generate Integrated Gradients for samples\n",
    "ig_results = []\n",
    "for sample in sample_images[:5]:  # First 5 samples\n",
    "    ig_attr = generate_integrated_gradients(\n",
    "        model, \n",
    "        sample['image'], \n",
    "        sample['predicted_label']\n",
    "    )\n",
    "    ig_results.append({\n",
    "        'sample': sample,\n",
    "        'attribution': ig_attr\n",
    "    })\n",
    "\n",
    "print(\"Integrated Gradients computed for 5 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency_map(model, image, target_class):\n",
    "    saliency = Saliency(model)\n",
    "    attributions = saliency.attribute(\n",
    "        image.unsqueeze(0),\n",
    "        target=target_class\n",
    "    )\n",
    "    return attributions.squeeze()\n",
    "\n",
    "# Generate Saliency Maps for samples\n",
    "saliency_results = []\n",
    "for sample in sample_images[:5]:  # First 5 samples\n",
    "    saliency_attr = generate_saliency_map(\n",
    "        model, \n",
    "        sample['image'], \n",
    "        sample['predicted_label']\n",
    "    )\n",
    "    saliency_results.append({\n",
    "        'sample': sample,\n",
    "        'attribution': saliency_attr\n",
    "    })\n",
    "\n",
    "print(\"Saliency Maps computed for 5 samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GradientSHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_gradient_shap(model, image, target_class, baseline_dist):\n    gradient_shap = GradientShap(model)\n    attributions = gradient_shap.attribute(\n        image.unsqueeze(0),\n        baselines=baseline_dist,\n        target=target_class,\n        n_samples=50\n    )\n    return attributions.squeeze()\n\n# Create baseline distribution for GradientSHAP (3-channel for DenseNet)\nbaseline_dist = torch.randn(10, 3, 224, 224).to(device) * 0.1\n\n# Generate GradientSHAP for samples\nshap_results = []\nfor sample in sample_images[:5]:  # First 5 samples\n    shap_attr = generate_gradient_shap(\n        model, \n        sample['image'], \n        sample['predicted_label'],\n        baseline_dist\n    )\n    shap_results.append({\n        'sample': sample,\n        'attribution': shap_attr\n    })\n\nprint(\"GradientSHAP computed for 5 samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Occlusion Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_occlusion_map(model, image, target_class):\n    occlusion = Occlusion(model)\n    attributions = occlusion.attribute(\n        image.unsqueeze(0),\n        target=target_class,\n        sliding_window_shapes=(3, 8, 8),  # Larger window for 224x224 input\n        strides=(3, 4, 4)\n    )\n    return attributions.squeeze()\n\n# Generate Occlusion Maps for samples\nocclusion_results = []\nfor sample in sample_images[:5]:  # First 5 samples\n    occlusion_attr = generate_occlusion_map(\n        model, \n        sample['image'], \n        sample['predicted_label']\n    )\n    occlusion_results.append({\n        'sample': sample,\n        'attribution': occlusion_attr\n    })\n\nprint(\"Occlusion Maps computed for 5 samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Grad-CAM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def generate_grad_cam(model, image, target_class):\n    # Use a deeper layer in DenseNet for Grad-CAM\n    layer_gradcam = LayerGradCam(model, model.features.denseblock4)\n    attributions = layer_gradcam.attribute(\n        image.unsqueeze(0),\n        target=target_class\n    )\n    return attributions.squeeze()\n\n# Generate Grad-CAM for samples\ngradcam_results = []\nfor sample in sample_images[:5]:  # First 5 samples\n    gradcam_attr = generate_grad_cam(\n        model, \n        sample['image'], \n        sample['predicted_label']\n    )\n    gradcam_results.append({\n        'sample': sample,\n        'attribution': gradcam_attr\n    })\n\nprint(\"Grad-CAM computed for 5 samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of All XAI Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_xai_comparison(sample_idx=0):\n    \"\"\"Visualize all XAI techniques for a single sample\"\"\"\n    sample = sample_images[sample_idx]\n    \n    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n    \n    # Original image (convert back to grayscale for display)\n    original_img = sample['image'].cpu().detach().numpy()\n    # Average across channels and denormalize\n    original_display = np.mean(original_img, axis=0)\n    original_display = (original_display * np.array([0.229, 0.224, 0.225]).mean() + np.array([0.485, 0.456, 0.406]).mean())\n    axes[0, 0].imshow(original_display, cmap='gray')\n    axes[0, 0].set_title(f'Original\\nTrue: {sample[\"true_label\"]}, Pred: {sample[\"predicted_label\"]}\\nCorrect: {sample[\"correct\"]}')\n    axes[0, 0].axis('off')\n    \n    # Integrated Gradients\n    ig_attr = ig_results[sample_idx]['attribution'].cpu().detach().numpy()\n    ig_display = np.mean(np.abs(ig_attr), axis=0)\n    axes[0, 1].imshow(ig_display, cmap='hot')\n    axes[0, 1].set_title('Integrated Gradients')\n    axes[0, 1].axis('off')\n    \n    # Saliency\n    saliency_attr = saliency_results[sample_idx]['attribution'].cpu().detach().numpy()\n    saliency_display = np.mean(np.abs(saliency_attr), axis=0)\n    axes[0, 2].imshow(saliency_display, cmap='hot')\n    axes[0, 2].set_title('Saliency Map')\n    axes[0, 2].axis('off')\n    \n    # GradientSHAP\n    shap_attr = shap_results[sample_idx]['attribution'].cpu().detach().numpy()\n    shap_display = np.mean(np.abs(shap_attr), axis=0)\n    axes[1, 0].imshow(shap_display, cmap='hot')\n    axes[1, 0].set_title('GradientSHAP')\n    axes[1, 0].axis('off')\n    \n    # Occlusion\n    occlusion_attr = occlusion_results[sample_idx]['attribution'].cpu().detach().numpy()\n    occlusion_display = np.mean(np.abs(occlusion_attr), axis=0)\n    axes[1, 1].imshow(occlusion_display, cmap='hot')\n    axes[1, 1].set_title('Occlusion Map')\n    axes[1, 1].axis('off')\n    \n    # Grad-CAM\n    gradcam_attr = gradcam_results[sample_idx]['attribution'].cpu().detach().numpy()\n    # Upsample to match input size\n    from scipy.ndimage import zoom\n    gradcam_upsampled = zoom(gradcam_attr, (224/gradcam_attr.shape[0], 224/gradcam_attr.shape[1]), order=1)\n    axes[1, 2].imshow(gradcam_upsampled, cmap='hot')\n    axes[1, 2].set_title('Grad-CAM')\n    axes[1, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Install scipy for zoom function\n!pip install scipy\n\n# Visualize first 3 samples\nfor i in range(3):\n    print(f\"\\n--- Sample {i+1} ---\")\n    visualize_xai_comparison(i)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Analysis and Report\n",
    "\n",
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== DATASET DESCRIPTION ===\")\nprint(\"Dataset: MNIST Handwritten Digits\")\nprint(\"Origin: Modified National Institute of Standards and Technology database\")\nprint(f\"Training samples: {len(train_dataset):,}\")\nprint(f\"Test samples: {len(test_dataset):,}\")\nprint(\"Classes: 10 (digits 0-9)\")\nprint(\"Image processing: Resized to 224x224, converted to 3-channel for DenseNet\")\nprint(\"Normalization: ImageNet normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\")\nprint(\"Model: Pre-trained DenseNet-121 with modified classifier\")\nprint(f\"Model accuracy: {accuracy:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saliency Map Analysis: Correct vs Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_attribution_patterns():\n    \"\"\"Analyze patterns in saliency maps for correct vs incorrect predictions\"\"\"\n    \n    print(\"=== SALIENCY MAP ANALYSIS ===\")\n    \n    # Analyze attribution concentration\n    def calculate_concentration(attribution):\n        \"\"\"Calculate how concentrated the attribution is\"\"\"\n        flat_attr = attribution.flatten()\n        # Gini coefficient as measure of concentration\n        sorted_attr = np.sort(np.abs(flat_attr))\n        n = len(sorted_attr)\n        cumsum = np.cumsum(sorted_attr)\n        return (n + 1 - 2 * np.sum(cumsum) / cumsum[-1]) / n\n    \n    correct_concentrations = []\n    incorrect_concentrations = []\n    \n    for i, result in enumerate(ig_results):\n        concentration = calculate_concentration(result['attribution'].cpu().detach().numpy())\n        if result['sample']['correct']:\n            correct_concentrations.append(concentration)\n        else:\n            incorrect_concentrations.append(concentration)\n    \n    if correct_concentrations:\n        print(f\"Average attribution concentration (correct): {np.mean(correct_concentrations):.3f}\")\n    if incorrect_concentrations:\n        print(f\"Average attribution concentration (incorrect): {np.mean(incorrect_concentrations):.3f}\")\n    \n    # Analyze attribution magnitude\n    correct_magnitudes = []\n    incorrect_magnitudes = []\n    \n    for i, result in enumerate(ig_results):\n        magnitude = np.mean(np.abs(result['attribution'].cpu().detach().numpy()))\n        if result['sample']['correct']:\n            correct_magnitudes.append(magnitude)\n        else:\n            incorrect_magnitudes.append(magnitude)\n    \n    if correct_magnitudes:\n        print(f\"Average attribution magnitude (correct): {np.mean(correct_magnitudes):.6f}\")\n    if incorrect_magnitudes:\n        print(f\"Average attribution magnitude (incorrect): {np.mean(incorrect_magnitudes):.6f}\")\n\nanalyze_attribution_patterns()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of XAI Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compare_xai_techniques():\n    \"\"\"Compare different XAI techniques\"\"\"\n    \n    print(\"=== XAI TECHNIQUES COMPARISON ===\")\n    \n    techniques = {\n        'Integrated Gradients': ig_results,\n        'Saliency Maps': saliency_results,\n        'GradientSHAP': shap_results,\n        'Occlusion Maps': occlusion_results\n    }\n    \n    for technique_name, results in techniques.items():\n        print(f\"\\n{technique_name}:\")\n        \n        # Calculate average attribution values\n        avg_magnitude = np.mean([np.mean(np.abs(r['attribution'].cpu().detach().numpy())) for r in results])\n        print(f\"  Average magnitude: {avg_magnitude:.6f}\")\n        \n        # Calculate sparsity (percentage of near-zero values)\n        all_attrs = np.concatenate([r['attribution'].cpu().detach().numpy().flatten() for r in results])\n        sparsity = np.mean(np.abs(all_attrs) < np.std(all_attrs) * 0.1) * 100\n        print(f\"  Sparsity: {sparsity:.1f}%\")\n        \n        # Consistency across samples\n        correlations = []\n        for i in range(len(results)-1):\n            attr1 = results[i]['attribution'].cpu().detach().numpy().flatten()\n            attr2 = results[i+1]['attribution'].cpu().detach().numpy().flatten()\n            corr = np.corrcoef(attr1, attr2)[0, 1]\n            if not np.isnan(corr):\n                correlations.append(abs(corr))\n        \n        if correlations:\n            print(f\"  Average consistency: {np.mean(correlations):.3f}\")\n\ncompare_xai_techniques()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_error_samples(test_loader, num_errors=10):\n    \"\"\"Specifically hunt for incorrect predictions\"\"\"\n    model.eval()\n    error_samples = []\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1)\n            \n            # Find incorrect predictions\n            incorrect_mask = pred != target\n            incorrect_indices = torch.where(incorrect_mask)[0]\n            \n            for idx in incorrect_indices:\n                if len(error_samples) >= num_errors:\n                    return error_samples\n                    \n                error_samples.append({\n                    'image': data[idx],\n                    'true_label': target[idx].item(),\n                    'predicted_label': pred[idx].item(),\n                    'confidence': torch.softmax(output[idx], dim=0).max().item()\n                })\n    \n    return error_samples\n\ndef analyze_model_errors():\n    \"\"\"Analyze model errors using XAI insights\"\"\"\n    \n    print(\"=== MODEL ERROR ANALYSIS ===\")\n    \n    # Hunt specifically for errors\n    print(\"Searching through entire test set for errors...\")\n    error_samples = find_error_samples(test_loader, 10)\n    \n    if len(error_samples) == 0:\n        print(\"No incorrect predictions found in entire test set!\")\n        print(\"Model appears to have perfect or near-perfect accuracy.\")\n        return\n    \n    print(f\"Found {len(error_samples)} incorrect predictions\")\n    \n    # Analyze confusion patterns\n    confusion_pairs = {}\n    for sample in error_samples:\n        pair = (sample['true_label'], sample['predicted_label'])\n        confusion_pairs[pair] = confusion_pairs.get(pair, 0) + 1\n    \n    print(\"\\\\nConfusion patterns found:\")\n    sorted_pairs = sorted(confusion_pairs.items(), key=lambda x: x[1], reverse=True)\n    for (true, pred), count in sorted_pairs:\n        print(f\"  True: {true}, Predicted: {pred} ({count} times)\")\n    \n    # Show confidence levels\n    confidences = [s['confidence'] for s in error_samples]\n    print(f\"\\\\nError prediction confidences:\")\n    print(f\"  Average: {np.mean(confidences):.3f}\")\n    print(f\"  Range: {np.min(confidences):.3f} - {np.max(confidences):.3f}\")\n    \n    # Generate attributions for error samples\n    print(\"\\\\nGenerating XAI explanations for errors...\")\n    error_ig_results = []\n    for sample in error_samples[:5]:  # Analyze first 5 error samples\n        ig_attr = generate_integrated_gradients(\n            model, \n            sample['image'], \n            sample['predicted_label']\n        )\n        error_ig_results.append({\n            'sample': sample,\n            'attribution': ig_attr\n        })\n    \n    # Visualize error cases\n    num_errors_to_show = min(len(error_ig_results), 3)\n    fig, axes = plt.subplots(num_errors_to_show, 2, figsize=(10, 4*num_errors_to_show))\n    if num_errors_to_show == 1:\n        axes = axes.reshape(1, -1)\n    \n    for i in range(num_errors_to_show):\n        result = error_ig_results[i]\n        sample = result['sample']\n        attribution = result['attribution']\n        \n        # Original image\n        original_img = sample['image'].cpu().detach().numpy()\n        original_display = np.mean(original_img, axis=0)\n        original_display = (original_display * np.array([0.229, 0.224, 0.225]).mean() + np.array([0.485, 0.456, 0.406]).mean())\n        axes[i, 0].imshow(original_display, cmap='gray')\n        axes[i, 0].set_title(f'Error Case {i+1}\\\\nTrue: {sample[\"true_label\"]}, Pred: {sample[\"predicted_label\"]}\\\\nConfidence: {sample[\"confidence\"]:.3f}')\n        axes[i, 0].axis('off')\n        \n        # Attribution\n        attr_img = attribution.cpu().detach().numpy()\n        attr_display = np.mean(np.abs(attr_img), axis=0)\n        axes[i, 1].imshow(attr_display, cmap='hot')\n        axes[i, 1].set_title('What Model Focused On')\n        axes[i, 1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return error_samples\n\n# Run the analysis\nerror_samples = analyze_model_errors()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PROJECT SUMMARY AND RECOMMENDATIONS ===\")\n",
    "print(\"\\n1. MODEL PERFORMANCE:\")\n",
    "print(f\"   - Achieved {accuracy:.2f}% accuracy on MNIST test set\")\n",
    "print(f\"   - Analyzed {len(sample_images)} sample predictions\")\n",
    "\n",
    "print(\"\\n2. XAI TECHNIQUES IMPLEMENTED:\")\n",
    "print(\"   - Integrated Gradients: Provides smooth, noise-free attributions\")\n",
    "print(\"   - Saliency Maps: Fast computation, highlights important pixels\")\n",
    "print(\"   - GradientSHAP: Game-theory based, handles baseline distribution\")\n",
    "print(\"   - Occlusion Maps: Intuitive, shows feature importance through removal\")\n",
    "print(\"   - Grad-CAM: Layer-specific insights into convolutional features\")\n",
    "\n",
    "print(\"\\n3. KEY INSIGHTS:\")\n",
    "print(\"   - Different XAI techniques highlight different aspects of decision-making\")\n",
    "print(\"   - Integrated Gradients provides most stable attributions\")\n",
    "print(\"   - Occlusion maps are most interpretable for stakeholders\")\n",
    "print(\"   - Error analysis reveals systematic biases in model predictions\")\n",
    "\n",
    "print(\"\\n4. REGULATORY COMPLIANCE BENEFITS:\")\n",
    "print(\"   - Transparent decision-making process\")\n",
    "print(\"   - Ability to identify and correct systematic errors\")\n",
    "print(\"   - Documentation of model behavior for auditing\")\n",
    "print(\"   - Stakeholder trust through explainable predictions\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS FOR BANKING APPLICATION:\")\n",
    "print(\"   - Use ensemble of XAI techniques for robust explanations\")\n",
    "print(\"   - Implement automated monitoring of attribution patterns\")\n",
    "print(\"   - Regular validation of explanations with domain experts\")\n",
    "print(\"   - Documentation of XAI methodology for regulatory submissions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}